# Real-time ASL Interpreter

This project is a Real-time American Sign Language (ASL) Interpreter that uses computer vision and machine learning techniques to recognize ASL gestures and translate them into text and speech and vice-versa using 3D avatar. The application is built using PyQt5 for the GUI, OpenCV for image processing, and a custom-trained neural network model for gesture classification.

# Features

- Real-time ASL gesture recognition
- Displays recognized gestures as text
- Text-to-speech functionality for recognized gestures
- Screenshot-based gesture recognition
- Real-time text and audio translation to ASL gestures using an 3D avatar
- Interactive GUI built with PyQt
